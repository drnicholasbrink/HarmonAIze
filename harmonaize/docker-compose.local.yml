volumes:
  harmonaize_local_postgres_data: {}
  harmonaize_local_postgres_data_backups: {}
  harmonaize_local_redis_data: {}
  nominatim_data: {}
  nominatim_flatnode: {}

services:
  django: &django
    build:
      context: .
      dockerfile: ./compose/local/django/Dockerfile
    image: harmonaize_local_django
    container_name: harmonaize_local_django
    depends_on:
      - postgres
      - redis
      - mailpit
    volumes:
      - .:/app:z
    env_file:
      - ./.envs/.local/.django
      - ./.envs/.local/.postgres
    ports:
      - '8000:8000'
    command: /start

  postgres:
    build:
      context: .
      dockerfile: ./compose/production/postgres/Dockerfile
    image: harmonaize_production_postgres
    container_name: harmonaize_local_postgres
    volumes:
      - harmonaize_local_postgres_data:/var/lib/postgresql/data
      - harmonaize_local_postgres_data_backups:/backups
    env_file:
      - ./.envs/.local/.postgres
    ports:
      - '5432:5432'

  mailpit:
    image: docker.io/axllent/mailpit:latest
    container_name: harmonaize_local_mailpit
    ports:
      - "8025:8025"

  redis:
    image: docker.io/redis:6
    container_name: harmonaize_local_redis
    volumes:
      - harmonaize_local_redis_data:/data

  celeryworker:
    <<: *django
    image: harmonaize_local_celeryworker
    container_name: harmonaize_local_celeryworker
    depends_on:
      - redis
      - postgres
      - mailpit
    ports: []
    command: /start-celeryworker

  celerybeat:
    <<: *django
    image: harmonaize_local_celerybeat
    container_name: harmonaize_local_celerybeat
    depends_on:
      - redis
      - postgres
      - mailpit
    ports: []
    command: /start-celerybeat

  flower:
    <<: *django
    image: harmonaize_local_flower
    container_name: harmonaize_local_flower
    ports:
      - '5555:5555'
    command: /start-flower

  
  nominatim:
    image: mediagis/nominatim:4.4
    container_name: harmonaize_local_nominatim
    environment:
      - NOMINATIM_PASSWORD=harmonaize_secure_pass_2024
      
      # AUTO-DOWNLOAD: Full Africa continent (WARNING: Large dataset!)
      - DOWNLOAD_PBF=https://download.geofabrik.de/africa-latest.osm.pbf
      

      # Import settings optimized for large dataset
      - UPDATE_MODE=import
      - IMPORT_STYLE=street      # Includes health facilities
      - IMPORT_WIKIPEDIA=false
      - IMPORT_US_POSTCODES=false
      - IMPORT_GB_POSTCODES=false
      - IMPORT_TIGER_ADDRESSES=false
      
      # Enable full geocoding (not just reverse)
      - REVERSE_ONLY=false
      - FREEZE=false
      
      # Performance settings for large dataset
      - THREADS=8                # Increased for large dataset
      - GUNICORN_WORKERS=4       # Increased for better performance
      
      # Memory settings optimized for Africa dataset (requires 16GB+ RAM)
      - POSTGRES_SHARED_BUFFERS=4GB           # Increased from 1GB
      - POSTGRES_MAINTENANCE_WORK_MEM=8GB     # Increased from 2GB  
      - POSTGRES_AUTOVACUUM_WORK_MEM=2GB      # Increased from 1GB
      - POSTGRES_WORK_MEM=512MB               # Increased from 256MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=12GB    # Increased from 4GB
      - POSTGRES_SYNCHRONOUS_COMMIT=off
      - POSTGRES_MAX_WAL_SIZE=4GB             # Increased from 1GB
      - POSTGRES_CHECKPOINT_TIMEOUT=20min     # Increased from 10min
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=64MB             # Increased from 32MB
      - POSTGRES_RANDOM_PAGE_COST=1.1
      
      # Additional settings for large import
      - POSTGRES_MIN_WAL_SIZE=2GB
      - POSTGRES_CHECKPOINT_SEGMENTS=128
      
    ports:
      - "8080:8080"
    volumes:
      - nominatim_data:/var/lib/postgresql/14/main
      - nominatim_flatnode:/nominatim/flatnode
      # No local PBF file needed - downloads automatically from Geofabrik
    shm_size: '8gb'  # Increased from 2gb for large dataset
    deploy:
      resources:
        limits:
          memory: 20G    # Increased from 6G - Africa dataset needs more RAM
        reservations:
          memory: 8G     # Increased from 2G
    
    # Health check with longer timeout for large dataset
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/status"]
      interval: 60s      # Check every minute
      timeout: 30s       # Longer timeout
      retries: 10        # More retries
      start_period: 7200s # 2 hours startup time for large dataset